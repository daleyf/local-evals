# ğŸ§ª Build Your Own Evals for Local LLMs

Test and benchmark **any** local large language model (LLM) with your own datasets.  
Easily compare models, track results over time, and design custom evaluation pipelines.

---

## ğŸš€ Features
- **Plug & play model testing** â€” works with `Ollama`, `transformers`, `llama.cpp`, or any local model API.
- **Custom datasets** â€” add your own `.jsonl` or `.csv` files for evaluation.
- **Example tests included** â€” start with built-in QA, summarization, and reasoning evals.
- **Simple metrics** â€” accuracy, BLEU, ROUGE, and custom scoring hooks.
- **Repeatable results** â€” same tests, different models, comparable outputs.

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourname/local-llm-evals.git
cd local-llm-evals
pip install -r requirements.txt
ğŸ—‚ Project Structure
perl
Copy
Edit
local-llm-evals/
â”œâ”€â”€ datasets/          # Example datasets (JSONL, CSV)
â”œâ”€â”€ tests/             # Example test scripts
â”œâ”€â”€ results/           # Saved run outputs + metrics
â”œâ”€â”€ evals/             # Core evaluation functions
â”œâ”€â”€ config.yaml        # Config for model + dataset selection
â””â”€â”€ README.md
âš¡ Quick Start
Run a test with the default dataset and model:

bash
Copy
Edit
python run_eval.py --model ollama:llama3 --dataset datasets/simple_qa.jsonl
Example Output:

yaml
Copy
Edit
Model: llama3 (Ollama)
Dataset: simple_qa
Accuracy: 83%
Avg Latency: 1.8s
ğŸ§© Adding Your Own Dataset
Create a .jsonl file in datasets/:

json
Copy
Edit
{"prompt": "What is the capital of France?", "expected": "Paris"}
{"prompt": "2 + 2 =", "expected": "4"}
Run:

bash
Copy
Edit
python run_eval.py --model ollama:mistral --dataset datasets/my_dataset.jsonl
ğŸ”¬ Example Tests
We include a few out of the box:

simple_qa.jsonl â€” Basic factual recall.

math_reasoning.jsonl â€” Step-by-step math reasoning.

summarization.jsonl â€” Text summarization scoring.

ğŸ“Š Metrics
By default, we compute:

Exact Match for QA

String Similarity (Levenshtein)

ROUGE / BLEU for summarization

You can add your own metrics in evals/metrics.py.

ğŸ”Œ Supported Backends
Ollama

HuggingFace transformers (with AutoModelForCausalLM)

llama.cpp

Any custom API endpoint

Switch models via:

bash
Copy
Edit
python run_eval.py --model transformers:meta-llama/Llama-2-7b-chat-hf
ğŸ“… Roadmap
 Add multi-turn conversation evals

 Add leaderboard-style web UI

 Add noise injection / robustness tests

ğŸ¤ Contributing
Pull requests welcome!

Fork it

Create your feature branch

Submit a PR


